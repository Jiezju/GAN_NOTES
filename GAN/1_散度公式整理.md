## 散度

- **f 散度 - 广义散度公式**

	$$D_f(P||Q) = \int_x q(x) f(\frac {p(x)}{q(x)}) dx$$
    
    其中，$f$ 表示一个函数超参数

	当 $f$ 满足以下两个两个条件，就可以使用 $D_f(P||Q)$ 衡量两种概率分布的差异
   - $f$ 函数是一个凸函数
   - $f(1) = 0$
   
   上述条件满足就是满足了 $f$ 散度 非负
   
   证明：使用 **Jensen** 不等式
   		
        $$E(f(x)) \ge f(E(x))$$
        
        则有：
        
        $$D_f(P||Q) = \int_x q(x) f(\frac {p(x)}{q(x)}) dx = E_{x-q(x)}[ f(\frac {p(x)}{q(x)}] \ge f[ E_{x-q(x)}[\frac {p(x)}{q(x)}]]$$
        
        $$ f[ E_{x-q(x)}[\frac {p(x)}{q(x)}]] = f(\int_x q(x) \cdot \frac{p(x)}{q(x)}dx) = f(1)$$
            
   - 当 $f(x) = xlogx$，$f$ 散度 <==> **KL散度**

		$$D_f(P||Q) = \int_x p(x)log(\frac {p(x)}{q(x)})dx$$
        
   - 当 $f(x) = -(x+1)log{\frac {1+x}{2}} + xlogx$，$f$ 散度 <==> **JS散度**

		$$D_f(P||Q) = \frac {1}{2}\int_x p(x)log(\frac {2p(x)}{q(x) + p(x)}) + q(x)log(\frac {2q(x)}{q(x) + p(x)})dx$$
        
        
- GAN 的基本公式

	$$min_G max_D [E_{x-P_{data}}[logD(x)] + E_{z-P_z}[log(1-D(G(z)))]]$$  
    
    - 对于判别器而言，输入真实数据$x$，希望$D(x) -> 1$，而对于生成器输出的$G(z)$，则希望，$D(G(z))->0$，所以希望最大化$D$
    - 对于生成器而言，则希望$D(G(z)) -> 1$，则与生成器相关的只有最后一项
    - $min_G max_D$ 的内涵是就是先从判别器的角度优化（最大化），然后再从生成器的角度优化（最小化）
    - GAN的目标是生成器能够学习到真实数据的分布，最终让一个优秀的判别器趋于0.5 